%!TEX TS-program = xelatex

% Шаблон документа LaTeX создан в 2018 году
% Алексеем Подчезерцевым
% В качестве исходных использованы шаблоны
% 	Данилом Фёдоровых (danil@fedorovykh.ru) 
%		https://www.writelatex.com/coursera/latex/5.2.2
%	LaTeX-шаблон для русской кандидатской диссертации и её автореферата.
%		https://github.com/AndreyAkinshin/Russian-Phd-LaTeX-Dissertation-Template

\documentclass[a4paper,14pt]{article}

\input{data/preambular.tex}
\begin{document} % конец преамбулы, начало документа
\input{data/title.tex}

\section*{Аннотация}

Количество страниц работы: \getlastpage, иллюстраций: \totalfigures, таблиц: \totaltables, источников: \LastBib.

%Объектом исследования является система управления обучением, позволяющая оптимизировать работу преподавателя. 
%Цель работы -- создание платформы для дистанционного и очного обучения для школьников. 
%В процессе работы проанализированы существующие системы дистанционного образования, настроена работа системы Moodle, настроена связь с серверами расписания учебных занятий и созданы собственные странички, интегрированные в Moodle. 
%Было произведено многократное ручное тестирование. 
%Предполагается использование результатов данной работы в инженерных классах МИЭМ НИУ ВШЭ.
Abstract—This work is considered the models of vector representation of the words. In particular, the possibility of affine transformations in the semantic space of the BERT model. The main source of texts for embeddings is the online library Cyberleninka. The main task was to identify the possibility of these transformations. To solve this problem, we used vector representations of words obtained by averaging the vectors of the same words in different contexts. Studies have shown that affine transformations of words are possible, but with low quality.

%\pagebreak

\section*{Annotation}

Total number of pages is \getlastpage, figures: \totalfigures, tables: \totaltables, cite sources: \LastBib.

Abstract—This work is considered the models of vector representation of the words. In particular, the possibility of affine transformations in the semantic space of the BERT model. The main source of texts for embeddings is the online library Cyberleninka. The main task was to identify the possibility of these transformations. To solve this problem, we used vector representations of words obtained by averaging the vectors of the same words in different contexts. Studies have shown that affine transformations of words are possible, but with low quality.

\pagebreak

\tableofcontents
\pagebreak

\section*{Введение}

В последние годы технологии машинного обучения стали неотъемлемой частью нашей жизни. 
Они представлены голосовыми помощниками, рекомендательными системами, умными домами, умными автомобилями и другими системами.
Важной частью этих систем являются модули, которые помогают сделать понятным для компьютера то, что от него требуется.
Для систем по обработке текста это модули обработки естественного языка или Natural Language Processing (NLP).

Компьютер без дополнительной помощи не способен обрабатывать естественный текст, зато компьютер хорошо работает с числами.
Поэтому для того, чтобы <<подружить>> вычислительную машину с текстом, нужно представить текст в виде чисел или в виде многомерных векторов.
Эти вектора также называют эмбеддингами.

Модели, использующие данный принцип, называются моделями векторного представления слов.
В основе большей части данных моделей лежит гипотеза дистрибутивности [9].
Эта гипотеза заключается в том, что слова со схожим смыслом встречаются в похожих контекстах.

Прорывной и наиболее известной моделью векторного представления слов является выпущенная в 2013 году модель word2vec [6].
Было представлено две архитектуры модели нейронной сети word2vec: Continuous Bag-of-Words
(CBOW) и Skip-gram. 
Continuous Bag-of-Words дословно переводится как <<непрерывный мешок слов>>.
Работает архитектура похожим образом, предсказывается вероятность появления слова по его контексту в виде окна фиксированного размера.
Архитектура Skip-gram наоборот предсказывает вероятность появления контекста у заданного слова.
В процессе обучения модель корректирует веса между входным и скрытым слоем, которые в дальнейшем станут эмбеддингами слов.

Оказалось, что полученные векторные представления слов скрывают в себе семантические отношения между словами.
Это хорошо заметно на примере задачи по построению пропорциональной аналогии.
Эту задачу можно сформулировать так: <<какое слово \textit{d} относится к слову \textit{c} так, 
как слово \textit{b} относится к слову \textit{a}>>.
В модели word2vec это отношение можно выразить в виде разницы векторов.
Для слов \textit{a} и \textit{b} с соответствующими им векторами $v_a$ и $v_b$ вектор разности $v_a - v_b$ будет характеризовать семантическую связь между словами.
Тогда для решением задачи пропорциональной аналогии будет выражение $v_d - v_c = v_b - v_a$, где $v_d$ и $v_c$ эмбеддинги слов \textit{d} и \textit{c} соответственно.

Из полученного выражения получаем: $v_d = v_c + v_b - v_a$.
Но вероятность того, что полученный вектор $v_d$ совпадает с вектором какого-либо слова крайне мала, поэтому в качестве ответа берется слово с вектором наиболее близким к $v_d$, формула (\ref{solv_prop}).

\begin{equation}
	v_d = \underset{v'}{argmax} \cos (v', v_c + v_b - v_a)
	\label{solv_prop}
\end{equation}

Данная задача для модели word2vec исследовалась в работе . Где пришли к выводу, что эта модель не всегда дает правильный ответ для задачи пропорциональной аналогии.

В настоящее время появляется все больше моделей для обработки естественного языка. Среди них можно выделить две модели ELMO и BERT.

модель 

Обе эти модели контекстуальные, это значит, что векторной представление одного и того же слова будет отличаться в зависимости от его контекста.




Целью данной практики является исследование аффинных преобразований для модели BERT и определение точности этих преобразований.
Для достижения поставленной цели потребовалось решить следующие задачи:

\begin{enumerate}
	
\item Исследование моделей векторного представления слов;
 
\item Исследование методов оценки аффинных преобразований;

\item Разработка метода оценки точности параллельного переноса для контекстуализированных моделей;

\item Подготовка экспериментальных данных;

\item Проведение экспериментов;

\item Оценка полученных результатов.

\end{enumerate}

\pagebreak

\section{Модели векторного представления}



Самой простой реализацией модели векторного представления слов является one-hot encoding.
Идея этой модели заключается в том, что в наборе из \textit{K} слов каждому слову сопоставить вектор длиной \textit{K} со всеми нулями и одной единицей в позиции \textit{i}, где \textit{i} - это номер слова во всем наборе.
Недостатком этого метода является то, что по данным векторным представлениям нельзя судить о семантической схожести слов.
Также для больших наборов слов длина векторных представлений будет очень большой, из-за эти представления неэффективно хранить в памяти





% Конец заключения
% ==============================================================================

\newpage 
\renewcommand{\refname}{{\normalsize СПИСОК ИСПОЛЬЗОВАННЫХ ИСТОЧНИКОВ}} 
\centering 
\begin{thebibliography}{9} 
	\addcontentsline{toc}{section}{\refname} 
	\bibitem{extraEdu} Концепция дополнительного образования в России, URL: \url{https://rg.ru/2014/09/08/obrazovanie-site-dok.html} (дата обращения 04.01.2019)

	
\end{thebibliography}

\end{document} % конец документа

