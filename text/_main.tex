%!TEX TS-program = xelatex
\documentclass[a4paper,14pt]{article}

\input{data/preambular.tex}
\begin{document} 	
	
\input{data/title.tex}
	
\section*{\normalsize \hfill Аннотация \hfill}

\sloppy

\begin{center}
	A Software Tool for Automatic Generation of a Graph of Conversation Using a Drama Corpus
\end{center}
\section*{\normalsize \hfill Abstract \hfill}


\newpage

\tableofcontents
\pagebreak

\section*{Введение}
\addcontentsline{toc}{section}{\protect\numberline{}Введение}


В последние годы технологии машинного обучения стали неотъемлемой частью нашей жизни. 
Они представлены голосовыми помощниками, рекомендательными системами, умными домами, умными автомобилями и другими системами.
Важной частью этих систем являются модули, которые помогают сделать понятным для компьютера то, что от него требуется.
Для систем по обработке текста это модули обработки естественного языка или Natural Language Processing (NLP).

Компьютер без дополнительной помощи не способен обрабатывать естественный текст, зато компьютер хорошо работает с числами.
Поэтому для того, чтобы <<подружить>> вычислительную машину с текстом, нужно представить текст в виде чисел или в виде многомерных векторов.
Эти вектора также называют эмбеддингами.

Модели, использующие данный принцип, называются моделями векторного представления слов.
В основе большей части данных моделей лежит гипотеза дистрибутивности [9].
Эта гипотеза заключается в том, что слова со схожим смыслом встречаются в похожих контекстах.

Прорывной и наиболее известной моделью векторного представления слов является выпущенная в 2013 году модель word2vec [6].
Было представлено две архитектуры модели нейронной сети word2vec: Continuous Bag-of-Words
(CBOW) и Skip-gram. 
Continuous Bag-of-Words дословно переводится как <<непрерывный мешок слов>>.
Работает архитектура похожим образом, предсказывается вероятность появления слова по его контексту в виде окна фиксированного размера.
Архитектура Skip-gram наоборот предсказывает вероятность появления контекста у заданного слова.
Порядок слов в контексте не влияет на результат ни в одном из этих алгоритмов.
В процессе обучения модель корректирует веса между входным и скрытым слоем, которые в дальнейшем станут эмбеддингами слов.

Оказалось, что полученные векторные представления слов скрывают в себе семантические отношения между словами.
Это хорошо заметно на примере задачи по построению пропорциональной аналогии.
Эту задачу можно сформулировать так: <<какое слово \textit{d} относится к слову \textit{c} так, 
как слово \textit{b} относится к слову \textit{a}>>.
В модели word2vec это отношение можно выразить в виде разницы векторов.
Для слов \textit{a} и \textit{b} с соответствующими им векторами $v_a$ и $v_b$ вектор разности $v_a - v_b$ будет характеризовать семантическую связь между словами.
Тогда для решением задачи пропорциональной аналогии будет выражение $v_d - v_c = v_b - v_a$, где $v_d$ и $v_c$ эмбеддинги слов \textit{d} и \textit{c} соответственно.

Из полученного выражения получаем: $v_d = v_c + v_b - v_a$.
Но вероятность того, что полученный вектор $v_d$ совпадает с вектором какого-либо слова крайне мала, поэтому в качестве ответа берется слово с вектором наиболее близким к $v_d$, формула (\ref{solv_prop}).

\begin{equation}
	v_d = \underset{v'}{argmax} \cos (v', v_c + v_b - v_a)
	\label{solv_prop}
\end{equation}

Данная задача для модели word2vec исследовалась в работе []. Где пришли к выводу, что эта модель не всегда дает правильный ответ для задачи пропорциональной аналогии.

В настоящее время появляется все больше моделей для обработки естественного языка.

В 2018 году в исследовании [] была предложена модель контекстуализированной модели обработки естественного языка ELMO (Embeddings from Language Models).
Если в модели word2vec векторное представление слов было одним и тем же независимо от контекста, то в ELMO решается эта проблема.
Для каждого контекста будет свой эмбеддинг.
В основе архитектуры ELMO лежат блоки долгой краткосрочной памяти (LSTM - Long Short-Term Memory).
Данные блоки расположены в прямом и обратном направлениях для того, чтобы при создании эмбеддинга учитывался контекст до и после слова.

Вскоре после выхода ELMO вышла модель BERT или Bidirectional Encoder Representations from Transformers.
BERT – это модель, побившая несколько рекордов по успешности решения ряда NLP-задач, например BERT показала лучшее качество на тесте SQuAD 1.1 [5].
BERT также контекстуализированная модель.
Архитектура модели BERT представляет из себя последовательность двунаправленных кодировщиков из Transformer.
В основе обучения модели лежат две идеи:

\begin{enumerate}
	\itemsep0em 
	\item Первая состоит в том, чтобы заменить 15\% текста масками и заставить модель предсказывать пропущенные слова.
	\itemsep0em 
	\item Вторая идея заключается в том, чтобы научить модель оценивать насколько одно предложение является логичным продолжением второго.
\end{enumerate}

Успех модели, помимо хорошего качества, можно объяснить тем, что код модели был выложен в открытый доступ, а также были выложены различные модели, предобученные на больших объемах данных.
Это дало возможность всем разработчикам встроить модель BERT в свои модели машинного обучения для обработки естественного языка.
%Вскоре после выхода статьи, описывающей модель, команда разработчиков также выложила в открытый доступ код модели и сделала возможным скачивание различных версий BERT, которые уже были предобучены на больших наборах данных.
%Этот знаменательный шаг позволил любому разработчику встраивать в свои модели машинного обучения для обработки естественного языка уже готовый мощный компонент, сохраняя свои время, энергию и ресурсы, необходимые для обучения модели обработки языка с нуля.
%Среди них можно выделить две модели ELMO и BERT.

ELMO и BERT - контекстуализированные модели, это значит, что векторное представление одного и того же слова будет отличаться в зависимости от его контекста.
Отсюда возникает вопрос, возможно ли провести аффинные преобразования в семантическом пространстве модели BERT?

\textbf{Целью} данной практики является исследование аффинных преобразований для модели BERT и определение точности этих преобразований.

Для достижения поставленной цели потребовалось решить следующие \textbf{задачи}:

\begin{enumerate}
	\itemsep0em 
	\item Исследование моделей векторного представления слов;
		\itemsep0em 
	\item Исследование методов оценки аффинных преобразований;
		\itemsep0em 
	\item Разработка метода оценки точности параллельного переноса для контекстуализированных моделей;
		\itemsep0em 
	\item Подготовка экспериментальных данных;
		\itemsep0em 
	\item Проведение экспериментов;
		\itemsep0em 
	\item Оценка полученных результатов.

\end{enumerate}

\newpage




\section{Обзор литературы}

\subsection{История развития моделей векторного представления}


Самой простой реализацией модели векторного представления слов является one-hot encoding.
Идея этой модели заключается в том, что в наборе из \textit{K} слов каждому слову сопоставить вектор длиной \textit{K} со всеми нулями и одной единицей в позиции \textit{i}, где \textit{i} - это номер слова во всем наборе.
Недостатком этого метода является то, что по данным векторным представлениям нельзя судить о семантической схожести слов.
Также для больших наборов слов длина векторных представлений будет очень большой, из-за эти представления неэффективно хранить в памяти



\subsection{Выводы}

\newpage

\section{Теоретическая часть}
\subsection{Выводы}
\begin{enumerate}
	\itemsep0em 
	\item 
\end{enumerate}

\newpage
\section{Практическая часть}

\subsection{Выводы}
\begin{enumerate}
	\itemsep0em 
	\item 
\end{enumerate}

\newpage
\section{Заключение}

\newpage 
\renewcommand{\refname}{{\normalsize \hfill Список использованных источников \hfill}} 
\bibliographystyle{unsrt}
\begin{thebibliography}{30}
	\addcontentsline{toc}{section}{Список использованных источников} 
% Введение
\bibitem{infro1}

\end{thebibliography}

\newpage



\end{document}