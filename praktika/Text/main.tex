%!TEX TS-program = xelatex

% Шаблон документа LaTeX создан в 2018 году
% Алексеем Подчезерцевым
% В качестве исходных использованы шаблоны
% 	Данилом Фёдоровых (danil@fedorovykh.ru) 
%		https://www.writelatex.com/coursera/latex/5.2.2
%	LaTeX-шаблон для русской кандидатской диссертации и её автореферата.
%		https://github.com/AndreyAkinshin/Russian-Phd-LaTeX-Dissertation-Template

\documentclass[a4paper,14pt]{article}

\input{data/preambular.tex}
\begin{document} % конец преамбулы, начало документа
	 \input{data/title.tex}
	
	\tableofcontents
	
	\pagebreak
	%\section{Индивидуальное задание}
	
	%\pagebreak
	
	%-
	
	%\pagebreak
	
	%\section{Дневник практики}
	
	%\pagebreak
	
	%-
	
	%\pagebreak
	
	
	\section{Введение}
	
	Целью данной работы является исследование и измерение качества аффинных преобразований для модели BERT.
	
	Для достижения поставленной цели необходимо решить следующие задачи:
	
	\begin{itemize}
	
		\item Исследование моделей векторного представления слов;
		
		\item Исследование методов оценки аффинных преобразований;
		
		\item Постановка задачи пропорциональной аналогии в терминах аффинного преобразования;
		
		\item Разработка метода оценки точности параллельного переноса для контекстуализированных моделей;
		
		\item Подготовка экспериментальных данных;
		
		\item Проведение экспериментов;
		
		\item Оценка полученных результатов.
		
	\end{itemize}

	Исследование проводится на языке python в среде Jupyter Notebok при использовании Google Colab. Jupyter Notebok является наиболее удобной платформой для проведения исследований на python.
	Google Colab является бесплатной и мощной платформой для запуска кода.
	При этом дается 12Гб оперативной памяти, доступ к Google диску для доступа к данным, а также есть возможность запускать код с использованием GPU.
	
	\pagebreak
	\section{Содержательная часть}
	
	\subsection{Описание профессиональных задач студента}
	
	Исследование моделей векторного представления слов;
	
	Исследование методов оценки афинных преобразований;
	
	Постановка задачи пропорциональной аналогии в терминах аффинного преобразования;
	
	Разработка метода оценки точности параллельного переноса для контекстуализированных моделей;
	
	Подготовка экспериментальных данных;
	
	Проведение экспериментов;
	
	Оценка полученных результатов.
	
	
	\subsection{Исследование моделей векторного представления слов}
	
	Векторное представление слов — метод обработки естественного языка, в основе которого лежит идея представить каждое слово или токен в виде вектора определенной размерности.
	
	\subsubsection{One-hot encoding}
	
	Самой простой реализацией модели векторного представления слов является one-hot encoding.
	Идея этой модели заключается в том, что в наборе из $K$ слов каждому слову соответствует вектор длиной $K$ со всеми нулями и единицей с позицией $i$, где $i$ - это номер слова во всем наборе.
	Недостатком этого метода является то, что по данным векторным представлениям нельзя судить о схожести слов.
	Также для больших наборов слов размер векторных представлений будет очень большим, из-за чего их неэффективно хранить в памяти.
	
	\subsubsection{Word2vec}
	
	Word2vec одна из первых моделей, использующих нейронные сети для создания векторных представлений слов.
	Идея создания векторов в word2vec основана на предположении о контекстной близости, а именно на том, что слова встречающиеся в одинаковых контекстах скорее всего имеют схожее значение.
	Предлагается проверять схожесть слов при помощи косинусного сходства их векторных представлений (\ref{eq:cos_sim}).
	
	\begin{equation}
		similarity(A,B) = cos(\theta) = \dfrac{A\cdot B}{\|A\| \|B\|}
		\label{eq:cos_sim}
	\end{equation}

	где
	
	$A$ - векторное представление первого слова
	
	$B$ - векторное представление второго слова
	
	$\theta$ - угол между векторами $A$ и $B$
	
	Существует 2 метода обучения word2vec: kip-gram и CBOW (Continuous Bag of Words).
	В Skip-gram по слову прогнозируется слова из его контекста, в CBOW, наоборот, по контексту предсказывается слово.
	В качестве выходного слоя в моделях применяется функция softmax в различных вариациях для того.
		
	\subsubsection{FastText}
	
	FastText является продолжением развития модели word2vec.
	При этом в fastText отличается от word2vec тем, что у новой модели используются N-граммы символов.
	Например, для слова молоко 3-граммами являются мол, оло, лок, око.
	Векторные представления строятся именно для N-грамм, векторные представления слов - это сумма векторных представлений всех его N-грамм.
	При этом решается проблема того, что словарь модели word2vec был ограничен.
	Использование N-грамм позволяет получать векторные представления для редких слов.
	
	\subsubsection{ELMO}
	
	ELMO была одной из первых моделей обработки естественного языка, которая учитывала контекст слова.
	Для моделей word2vec или fastText при вычислении векторного представления не учитывается контекст слова, для омонимов и омографов представления будут одинковыми.
	В ELMO решается эта проблема, в основе этой модели лежит многослойная двунаправленная рекуррентная нейронная сеть c LSTM (Long short-term memory).
	
	\subsubsection{BERT}
	
	Модель BERT или Bidirectional Encoder Representations from Transformers была опубликована командой Google AI в 2018 году.
	На момент появления модель BERT показала лучшее качество на тесте SQuAD 1.1.
	
	BERT состоит из 12 следующих друг за другом энкодеров.
	Каждый энкодер состоит из компонентов, первый компонент - это слой внутреннего внимания (self-attention), второй - нейронная сеть прямого распространения (feed-forward neural network).
	
	Обучение модели BERT основывается на следующих принципах.	
	Первый основывается на том, чтобы заменить 15\% слов масками и обучить модель предсказывать эти слова.
	Этот принцип позволяет модели самой обучаться на полноценных текстах без предварительной разметки, что позволило обучить BERT на огромном массиве данных.
	Вторая идея состоит в том, чтобы дополнительно научить BERT определять, является ли одно предложение логичным продолжением другого.
	
	\subsection{Исследование методов оценки аффинных преобразований}
	2
	
	\subsection{Постановка задачи пропорциональной аналогии в терминах аффинного преобразования}
	3
	
	\subsection{Разработка метода оценки точности параллельного переноса для контекстуализированных моделей}
	4
	
	\subsection{Подготовка экспериментальных данных}
	
	Для получения эмбеддингов слов были взяты тексты из электронной библиотеки КиберЛенинка.
	Тексты двух жанров: литература и политика.
	
	Для оценки качества аффинных преобразований используется датасет $Google\_analogy\_test\_set$.
	Данный датасет был переведен на русский язык с сохранением семантических отношений между словами.
	Не все слова из данного датасета есть в словаре BERT, поэтому часть отношений пришлось убрать.
	
	\subsection{Проведение экспериментов}
	
	Для проведения экспериментов используется язык программирования python в среде Jupyter Notebok с использованием Google Colab.
	Jupyter Notebok является наиболее удобной платформой для проведения исследований на python.
	Google Colab является бесплатной и мощной платформой для запуска кода.
	При этом дается 12Гб оперативной памяти, доступ к Google диску для доступа к данным, а также есть возможность запускать код с использованием GPU.
	В качестве фреймворка для работы с моделью BERT был выбран pytorch, так как это современная и гибкая библиотека для работы с глубинным обучением.
	
	Для проведения экспериментов необходимо подготовить данные для их обработки в модели BERT.
	Сначала весь текст разбивается на отдельные предложения, далее происходит их токенизация и индексация.
	На этом этапе обработанные предложения по-одному отправляются в модель BERT.
	Данным способом обработаны по 1 миллиону предложений для каждого жанра.
	
	Полученные после обработки объекты представляют из себя четырёхразмерные тензоры, где оси отражают следующую информацию (в скобках представлено количество элементов):
	
	\begin{enumerate}
		
		\item Номер слоя (13 слоев);
		
		\item Номер батча (1 предложение);
		
		\item Количество слов/токенов в предложении (количество токенов в предложении);
		
		\item Векторное представление (768 свойств).

	\end{enumerate}	

	По оси слоев первый слой - это эмбеддинг, поступающий на вход модели, остальные 12 слоев отображают выходы 12 энкодеров.
	Номер батча в нашем случае не важен, так как используется только одно предложение.
	Следующая ось отображает токены в предложении с сохранением порядка.
	Последняя ось отвечает за векторное представление каждого токена.
	
	Получить итоговое векторное представление для токена можно несколькими способами (рисунок \ref{fig:dif_vars_get_v}).
	В нашем случае используется способ с суммированием последних четырех слоев, данный способ показывает хорошее качество.
	Способ с конкатенацией последних четырех не используется, так как он требует в 4 раза больше ресурсов.
	
	Описанным ранее методом обрабатываются все подготовленные предложения.
	Обработка происходит пачками по 10 тысяч предложений.
	Векторные представления токенов каждой пачки сохраняются на Google диск.
	Сделано это из-за ограничений оперативной памяти устройства.
	
	После того как получены векторные представления для всего текста, считаются средние эмбеддинги для всех токенов.
	Из-за ограничений оперативной памяти нельзя посчитать сразу все векторные представления, поэтому они считаются порциями с сохранением промежуточных результатов.
	
	Далее проверялось семантические отношения полученных эмбеддингов на переведенном датасете $Google\_analogy\_test\_set$.
	
	\subsection{Оценка полученных результатов}
	
	На рисунках \ref{fig:liter} и \ref{fig:politics} представлены результаты 

	
	
	\pagebreak
	\section{Заключение}
	
	%В ходе практики, был изучен принцип работы сверточной нейронной сети,  рассмотрены самые популярные архитектуры сверточных нейронных сетей и подходы к их обучению.
	
	%Были приобретены навыки по поиску необходимого датасета, по работе со сверточными нейронными сетями, их дообучению и применению в реальном приложении.
	%Помимо этого, были получены навыки по работе с графическим интерфейсом в python.
	
	%По окончанию практики была достигнута главная цель - применение теоретических знаний, полученных в процессе обучения, при решении реальных задач.
	
	%А также приобретены навыки и опыт практической работы.
	%Данная практика является хорошим практическим опытом для дальнейшей самостоятельной деятельности.
	
	\pagebreak
	\section{Приложения}
	
	С кодом можно ознакомиться по ссылке:

 \href{https://github.com/andrsolo21/hse_Af_Tr_BERTn}{https://github.com/andrsolo21/hse\_Af\_Tr\_BERT}.
	

	
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{image/irvrsv9mefroz7io6ilnjng3fo4}
	\caption{Возможные варианты получения векторного представления}
	\label{fig:dif_vars_get_v}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{image/liter}
	\caption{Результаты тестирования для текста с литературой}
	\label{fig:liter}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{image/politics}
	\caption{Результаты тестирования для текста с политикой}
	\label{fig:politics}
\end{figure}


	%\newpage 
	%\renewcommand{\refname}{{\normalsize Список использованных источников}} 
	%\centering 
	%\begin{thebibliography}{9} 
	%	\addcontentsline{toc}{section}{\refname} 
	%	\bibitem{Verilog} Thomas D., Moorby P. The Verilog Hardware Description Language. – Springer Science \& Business Media, 2008.
	%	\bibitem{Quartus} Антонов А., Филиппов А., Золотухо Р. Средства системной отладки САПР Quartus II //Компоненты и технологии. – 2008. – №. 89.
	%\end{thebibliography}
	
\end{document} % конец документа